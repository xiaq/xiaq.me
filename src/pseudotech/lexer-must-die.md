lexer 和 parser 的划分一直是一个让我不能完全理解的事情。这一划分对于某些语言或许有其优势，但是对于很多现代编程语言早已不敷用了；所谓词法记号(lexical token) 在很多情况下已经成为了碍手碍脚的多余概念。

[C++ 的“>>”](http://en.wikipedia.org/wiki/C%2B%2B11#Right_angle_bracket)就是很经典的例子，单靠 lexer 无法确定这是两个右尖括号（用于关闭指定类型参数的两个左尖括号“<”）还是一个右移操作符。这时最好的做法是让 lexer 产生两个“>”，然后让 parser 来判断。也就是说，此时词法记号的界定已经取决于 parser 的状态，lexer 已经不能独立完成词法记号的划分。更糟糕的是，词法记号已经不再包含语义信息：“>”可能表示的是大于操作符、右尖括号或者右移操作符的一部分。lexer 作用之有限让人捉急。

另一个很好的例子是字符串内插（虽然我不觉得这是个好特性）。考虑 shell 的例子：```echo "hello `echo "world"`."```。注意 `world` 左边的引号；如果缺少语法信息，lexer 会认为这个字符串在这个地方结束了。因此在此例中，如果缺少 parser，lexer 根本无法确定字符串的边界了。这颠覆了“parser 调用 lexer”这一传统体位。比颠覆体位更糟糕的是，parser 和 lexer 产生了耦合，lexer 不能独立于 parser 开发和调试，分离成两个部件的优势几乎都丧失了。

抛弃 lexer 抽象并没有任何技术上的问题，因为正则语言是上下文无关语言的子集。我之前尝试写了一个 [jade](https://github.com/xiaq/jadepy/) 的 [Python 实现](https://github.com/xiaq/jadepy/)，手写 parser 没有用到 lexer，也没有遇到任何问题。

实际上，在很多字符含义对环境高度敏感的语言 (包括 jade) 中，因为不存在能适用于所有环境的词法划分法则，lexer 抽象不得不被抛弃。实际上即使在语法更规则的语言中，词法记号的内部也往往有其结构，把这个内部结构放到语法树中是很方便的。一个简单的例子就是 C 的浮点数字面量，它包含小数部分、可选的 e 或 E 加上指数部分、以及可选的类型后缀。把这些组成部分分别放进语法树可以简化之后的语义步骤。

词法记号在 parser 中一般称为终结符号 (terminal symbol)，以示其不包含具有语法意义的内部结构。更高的文法单元则都称为非终结符号 (nonterminal symbol)。但是上述论证已经说明了，终结符号和非终结符号的划分是任意的，而且所谓“终结符号”也可能包含具有语法意义的内部结构。

维基百科的 [scannerless parsing](http://en.wikipedia.org/wiki/Scannerless_parsing) 列出了三个指控，即难于调试、不确定性和歧义性。后面两个我没有 get 到它的点，但是难于调试这一指控绝对是不成立的。去掉了多余的 lexer 的抽象之后代码的内部接口得到了简化，调试起来反而会更简单。

lexer 作为一个陈旧的、需要抛弃的抽象，为何一直没有消失？我能想到两个原因。其一，由于形式语言理论来自于自然语言理论，早期计算机科学家也就照搬了语言学对形态(morphology) 和语法(syntax) 的划分。其二，最早流行的 parser 生成器 `yacc` 就是要和 lexer 生成器 `lex` 配合使用的，这一结构也就被后来的 parser 生成器一路沿袭下来了。

今天睡觉睡得浑浑噩噩，想必写得内容混乱，不知所云。

<!-- vi: se tw=0 nolbr: -->
